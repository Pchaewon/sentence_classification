{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/hom1/ict17/.conda/envs/stargan-v2/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://dacon.io/en/competitions/official/236037/overview/description\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for graphing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = pd.read_csv('./train.csv')\n",
    "train_original.drop(columns=['ID'], inplace=True)\n",
    "test = pd.read_csv('./test.csv')\n",
    "test.drop(columns=['ID'], inplace=True)\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "CFG = {\n",
    "    'EPOCHS':100,\n",
    "    'LEARNING_RATE':0.01,\n",
    "    'BATCH_SIZE':20,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정\n",
    "device = torch.device('cuda:4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, _, _ = train_test_split(train_original, train_original['label'], test_size=0.2, random_state=CFG['SEED'])\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_nm = 'klue/roberta-small'\n",
    "base_model = AutoModel.from_pretrained(model_nm)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log value : 90.4092368060602\n"
     ]
    }
   ],
   "source": [
    "tokenizer_len = [len(tokenizer(s)['input_ids']) for s in train['문장']]\n",
    "sns.histplot(tokenizer_len)\n",
    "plt.show()\n",
    "\n",
    "print(f'log value : {np.mean(tokenizer_len)+3*np.std(tokenizer_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log value : 4.8974082584991345\n",
      "original value : 133.94218592094492\n"
     ]
    }
   ],
   "source": [
    "tokenizer_log = np.log(tokenizer_len)\n",
    "sns.histplot(tokenizer_log)\n",
    "plt.show()\n",
    "\n",
    "print(f'log value : {np.mean(tokenizer_log)+3*np.std(tokenizer_log)}')\n",
    "print(f'original value : {np.exp(np.mean(tokenizer_log)+3*np.std(tokenizer_log))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTypeDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, labels=None):\n",
    "        texts = dataframe['문장'].values.tolist()\n",
    "\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=90, truncation=True, return_tensors='pt') for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            type_tmp = self.labels['type'][idx]\n",
    "            polarity_tmp = self.labels['polarity'][idx]\n",
    "            tense_tmp = self.labels['tense'][idx]\n",
    "            certainty_tmp = self.labels['certainty'][idx]\n",
    "            return text, torch.Tensor(type_tmp), torch.Tensor(polarity_tmp), torch.Tensor(tense_tmp), torch.Tensor(certainty_tmp)\n",
    "        else:\n",
    "            return text, torch.Tensor([-1,-1,-1,-1]), torch.Tensor([-1,-1,-1]), torch.Tensor([-1,-1,-1]), torch.Tensor([-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.klue = base_model # from transformers package\n",
    "\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.clf = nn.Linear(512,32)\n",
    "        self.type_clf = nn.Linear(32,4)\n",
    "        self.polarity_clf = nn.Linear(32,3)\n",
    "        self.tense_clf = nn.Linear(32,3)\n",
    "        self.certainty_clf = nn.Linear(32,2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids : token's id / attention_mask : make a model to focus on which token\n",
    "        klue_out = self.klue(input_ids= input_ids, attention_mask = attention_mask)[0][:,0]\n",
    "\n",
    "        x = self.fc1(klue_out)\n",
    "        x = self.relu(x)\n",
    "        x = self.clf(x)\n",
    "        x = self.relu(x)\n",
    "        type_output = self.type_clf(x)\n",
    "        type_output = self.softmax(type_output)\n",
    "        polarity_output = self.polarity_clf(x)\n",
    "        polarity_output = self.softmax(polarity_output)\n",
    "        tense_output = self.tense_clf(x)\n",
    "        tense_output = self.softmax(tense_output)\n",
    "        certainty_output = self.certainty_clf(x)\n",
    "        certainty_output = self.softmax(certainty_output)\n",
    "\n",
    "        return type_output, polarity_output, tense_output, certainty_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_train(model, train_dataloader, val_dataloader, learning_rate, epochs, model_nm):\n",
    "    best_val_loss = 99999999999999 # setting max (act as infinity)\n",
    "    early_stopping_threshold_count = 0\n",
    "\n",
    "    '''CrossEntropyLoss'''\n",
    "    # type\n",
    "    nSamples = [575, 13558, 257, 2151]#[0.2270, 0.2525, 0.2550, 0.2655]\n",
    "    normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "    normedWeights = torch.FloatTensor(normedWeights).to(device)\n",
    "\n",
    "    # polarity\n",
    "    nSamples2 = [15793, 183, 565]#[0.2270, 0.2525, 0.2550]\n",
    "    normedWeights2 = [1 - (x / sum(nSamples2)) for x in nSamples2]\n",
    "    normedWeights2 = torch.FloatTensor(normedWeights2).to(device)\n",
    "\n",
    "    # tense\n",
    "    nSamples3 = [8032, 1643, 6866]#[0.2270, 0.2525, 0.2550]\n",
    "    normedWeights3 = [1 - (x / sum(nSamples3)) for x in nSamples3]\n",
    "    normedWeights3 = torch.FloatTensor(normedWeights3).to(device)\n",
    "\n",
    "    # certainty\n",
    "    nSamples4 = [15192, 1349]#[0.2270, 0.2525]\n",
    "    normedWeights4 = [1 - (x / sum(nSamples4)) for x in nSamples4]\n",
    "    normedWeights4 = torch.FloatTensor(normedWeights4).to(device)\n",
    "    \n",
    "    criterion = {\n",
    "        'type' : nn.CrossEntropyLoss(normedWeights).to(device),\n",
    "        'polarity' : nn.CrossEntropyLoss(normedWeights2).to(device),\n",
    "        'tense' : nn.CrossEntropyLoss(normedWeights3).to(device),\n",
    "        'certainty' : nn.CrossEntropyLoss(normedWeights4).to(device)\n",
    "    }\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_train(model, train_dataloader, val_dataloader, learning_rate, epochs, model_nm):\n",
    "    best_val_loss = 99999999999999 # setting max (act as infinity)\n",
    "    early_stopping_threshold_count = 0\n",
    "\n",
    "    '''CrossEntropyLoss'''\n",
    "    # type\n",
    "    nSamples = [575, 13558, 257, 2151]#[0.2270, 0.2525, 0.2550, 0.2655]\n",
    "    normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "    normedWeights = torch.FloatTensor(normedWeights).to(device)\n",
    "\n",
    "    # polarity\n",
    "    nSamples2 = [15793, 183, 565]#[0.2270, 0.2525, 0.2550]\n",
    "    normedWeights2 = [1 - (x / sum(nSamples2)) for x in nSamples2]\n",
    "    normedWeights2 = torch.FloatTensor(normedWeights2).to(device)\n",
    "\n",
    "    # tense\n",
    "    nSamples3 = [8032, 1643, 6866]#[0.2270, 0.2525, 0.2550]\n",
    "    normedWeights3 = [1 - (x / sum(nSamples3)) for x in nSamples3]\n",
    "    normedWeights3 = torch.FloatTensor(normedWeights3).to(device)\n",
    "\n",
    "    # certainty\n",
    "    nSamples4 = [15192, 1349]#[0.2270, 0.2525]\n",
    "    normedWeights4 = [1 - (x / sum(nSamples4)) for x in nSamples4]\n",
    "    normedWeights4 = torch.FloatTensor(normedWeights4).to(device)\n",
    "    \n",
    "    criterion = {\n",
    "        'type' : nn.CrossEntropyLoss(normedWeights).to(device),\n",
    "        'polarity' : nn.CrossEntropyLoss(normedWeights2).to(device),\n",
    "        'tense' : nn.CrossEntropyLoss(normedWeights3).to(device),\n",
    "        'certainty' : nn.CrossEntropyLoss(normedWeights4).to(device)\n",
    "    }\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        model.train() # sets into the training mode\n",
    "        \n",
    "        for train_input, type_label, polarity_label, tense_label, certainty_label in tqdm(train_dataloader):\n",
    "            attention_mask = train_input['attention_mask'].to(device)\n",
    "            input_ids = train_input['input_ids'].squeeze(1).to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "            #print(f'type_label:{type_label}')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            type_output, polarity_output, tense_output, certainty_output = model(input_ids, attention_mask) # from the forward function\n",
    "            #print(f'type_output:{type_output}')\n",
    "            \n",
    "            loss = 0.25*criterion['type'](type_output, type_label.float()) + \\\n",
    "                   0.25*criterion['polarity'](polarity_output, polarity_label.float()) + \\\n",
    "                   0.25*criterion['tense'](tense_output, tense_label.float()) + \\\n",
    "                   0.25*criterion['certainty'](certainty_output, certainty_label.float())\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        with torch.no_grad(): # since we should not change gradient for validation \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            model.eval() # deactivate training\n",
    "            \n",
    "            # same process as the above\n",
    "            for val_input, vtype_label, vpolarity_label, vtense_label, vcertainty_label in tqdm(val_dataloader):\n",
    "                attention_mask = val_input['attention_mask'].to(device)\n",
    "                input_ids = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                vtype_label = vtype_label.to(device)\n",
    "                vpolarity_label = vpolarity_label.to(device)\n",
    "                vtense_label = vtense_label.to(device)\n",
    "                vcertainty_label = vcertainty_label.to(device)\n",
    "                \n",
    "                vtype_output, vpolarity_output, vtense_output, vcertainty_output = model(input_ids, attention_mask) # from the forward function\n",
    "\n",
    "                loss = 0.25*criterion['type'](vtype_output, vtype_label.float()) + \\\n",
    "                        0.25*criterion['polarity'](vpolarity_output, vpolarity_label.float()) + \\\n",
    "                        0.25*criterion['tense'](vtense_output, vtense_label.float()) + \\\n",
    "                        0.25*criterion['certainty'](vcertainty_output, vcertainty_label.float())\n",
    "\n",
    "                total_loss_val += loss.item()\n",
    "\n",
    "            \n",
    "            print(f'Epochs: {epoch + 1} '\n",
    "                  f'| Train Loss: {total_loss_train / len(train_dataloader): .3f} '\n",
    "                  f'| Train Accuracy: {total_acc_train / (len(train_dataloader.dataset)): .3f} '\n",
    "                  f'| Val Loss: {total_loss_val / len(val_dataloader): .3f} '\n",
    "                  f'| Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}')\n",
    "            \n",
    "            if best_val_loss > total_loss_val:\n",
    "                best_val_loss = total_loss_val # saving only the best one\n",
    "                torch.save(model, f\"model/{model_nm}.pt\")\n",
    "                print(\"Saved model\")\n",
    "                early_stopping_threshold_count = 0\n",
    "            else:\n",
    "                early_stopping_threshold_count += 1 # checking how many epochs have passed that val_loss didn't increase\n",
    "                \n",
    "            if early_stopping_threshold_count >= 3: # ==> patience=1\n",
    "                print(\"Early stopping\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>유형_대화형</th>\n",
       "      <th>유형_사실형</th>\n",
       "      <th>유형_예측형</th>\n",
       "      <th>유형_추론형</th>\n",
       "      <th>극성_긍정</th>\n",
       "      <th>극성_미정</th>\n",
       "      <th>극성_부정</th>\n",
       "      <th>시제_과거</th>\n",
       "      <th>시제_미래</th>\n",
       "      <th>시제_현재</th>\n",
       "      <th>확실성_불확실</th>\n",
       "      <th>확실성_확실</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>용산구청 관계자는 ＂재정이 열악한 지자체로서는 1800억원을 마련할 수 없다＂며 서...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>부산시는 이처럼 부산이 가파른 상승세를 보이는 이유에 대해 지난해부터 추진하고 있는...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그러나 미숙아, 만성호흡기질환, 선천 심장병, 선천 면역결핍질환, 암환자 등의 고위...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>탁구 종목에서 중국 대표팀 위상이 뛰어나기 때문이다.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이 논문에 따르면 ＇BT-11＇은 뇌의 신경전달물질인 아세틸콜린을 분해하는 효소의 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13227</th>\n",
       "      <td>우리가 익히 아는 대로 임꺽정은 신출귀몰했다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13228</th>\n",
       "      <td>김 상무보는 ＂실제 이용자 수와 인당 사용시간 등 주요 데이터가 매년 두 자릿수 상...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13229</th>\n",
       "      <td>＇디폴트 옵션＇의 필요성을 주장해온 쪽이 항상 사례로 들어온 것이 ＇401K＇로 불...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13230</th>\n",
       "      <td>1992년부터 선양시 조선족노인협회를 후원하기 시작해 1997년에는 1500㎡ 건물...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13231</th>\n",
       "      <td>차량은 고속 상태지만 운전자는 정체모드에서 사고가 많이 발생한다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13232 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장  유형_대화형  유형_사실형  \\\n",
       "0      용산구청 관계자는 ＂재정이 열악한 지자체로서는 1800억원을 마련할 수 없다＂며 서...       0       1   \n",
       "1      부산시는 이처럼 부산이 가파른 상승세를 보이는 이유에 대해 지난해부터 추진하고 있는...       0       1   \n",
       "2      그러나 미숙아, 만성호흡기질환, 선천 심장병, 선천 면역결핍질환, 암환자 등의 고위...       0       1   \n",
       "3                          탁구 종목에서 중국 대표팀 위상이 뛰어나기 때문이다.       0       0   \n",
       "4      이 논문에 따르면 ＇BT-11＇은 뇌의 신경전달물질인 아세틸콜린을 분해하는 효소의 ...       0       1   \n",
       "...                                                  ...     ...     ...   \n",
       "13227                          우리가 익히 아는 대로 임꺽정은 신출귀몰했다.       0       1   \n",
       "13228  김 상무보는 ＂실제 이용자 수와 인당 사용시간 등 주요 데이터가 매년 두 자릿수 상...       0       1   \n",
       "13229  ＇디폴트 옵션＇의 필요성을 주장해온 쪽이 항상 사례로 들어온 것이 ＇401K＇로 불...       1       0   \n",
       "13230  1992년부터 선양시 조선족노인협회를 후원하기 시작해 1997년에는 1500㎡ 건물...       0       1   \n",
       "13231               차량은 고속 상태지만 운전자는 정체모드에서 사고가 많이 발생한다.       0       1   \n",
       "\n",
       "       유형_예측형  유형_추론형  극성_긍정  극성_미정  극성_부정  시제_과거  시제_미래  시제_현재  확실성_불확실  \\\n",
       "0           0       0      1      0      0      1      0      0        0   \n",
       "1           0       0      1      0      0      1      0      0        0   \n",
       "2           0       0      1      0      0      0      0      1        0   \n",
       "3           0       1      1      0      0      0      0      1        0   \n",
       "4           0       0      1      0      0      0      0      1        0   \n",
       "...       ...     ...    ...    ...    ...    ...    ...    ...      ...   \n",
       "13227       0       0      1      0      0      1      0      0        0   \n",
       "13228       0       0      1      0      0      1      0      0        0   \n",
       "13229       0       0      1      0      0      0      0      1        0   \n",
       "13230       0       0      1      0      0      1      0      0        0   \n",
       "13231       0       0      1      0      0      0      0      1        0   \n",
       "\n",
       "       확실성_확실  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "13227       1  \n",
       "13228       1  \n",
       "13229       1  \n",
       "13230       1  \n",
       "13231       1  \n",
       "\n",
       "[13232 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tmp = train[['문장', '유형', '극성', '시제', '확실성']]\n",
    "train_tmp = pd.get_dummies(train_tmp, columns=['유형', '극성', '시제', '확실성'])\n",
    "train_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = train_tmp.iloc[:,1:5].values.tolist()\n",
    "train_polarity = train_tmp.iloc[:,5:8].values.tolist()\n",
    "train_tense = train_tmp.iloc[:,8:11].values.tolist()\n",
    "train_certainty = train_tmp.iloc[:,11:13].values.tolist()\n",
    "train_labels = {\n",
    "    'type': train_type,\n",
    "    'polarity': train_polarity,\n",
    "    'tense': train_tense,\n",
    "    'certainty': train_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp = val[['문장', '유형', '극성', '시제', '확실성']]\n",
    "val_tmp = pd.get_dummies(val_tmp, columns=['유형', '극성', '시제', '확실성'])\n",
    "\n",
    "val_type = val_tmp.iloc[:,1:5].values.tolist()\n",
    "val_polarity = val_tmp.iloc[:,5:8].values.tolist()\n",
    "val_tense = val_tmp.iloc[:,8:11].values.tolist()\n",
    "val_certainty = val_tmp.iloc[:,11:13].values.tolist()\n",
    "val_labels = {\n",
    "    'type': val_type,\n",
    "    'polarity': val_polarity,\n",
    "    'tense': val_tense,\n",
    "    'certainty': val_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(SentenceTypeDataset(train_tmp, tokenizer, train_labels), batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0) # num_workers: how many subprocesses to use for data loading  \n",
    "val_dataloader = DataLoader(SentenceTypeDataset(val_tmp, tokenizer, val_labels), batch_size=CFG['BATCH_SIZE'], num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceClassifier(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceClassifier(\n",
      "  (klue): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (clf): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (type_clf): Linear(in_features=32, out_features=4, bias=True)\n",
      "  (polarity_clf): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (tense_clf): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (certainty_clf): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662/662 [00:24<00:00, 26.82it/s]\n",
      "100%|██████████| 166/166 [00:01<00:00, 100.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.357 | Train Accuracy:  0.000 | Val Loss:  0.359 | Val Accuracy:  0.000\n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662/662 [00:24<00:00, 26.59it/s]\n",
      "100%|██████████| 166/166 [00:01<00:00, 98.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.356 | Train Accuracy:  0.000 | Val Loss:  0.359 | Val Accuracy:  0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662/662 [00:24<00:00, 26.69it/s]\n",
      "100%|██████████| 166/166 [00:01<00:00, 99.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.356 | Train Accuracy:  0.000 | Val Loss:  0.359 | Val Accuracy:  0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662/662 [00:24<00:00, 26.60it/s]\n",
      "100%|██████████| 166/166 [00:01<00:00, 99.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.356 | Train Accuracy:  0.000 | Val Loss:  0.359 | Val Accuracy:  0.000\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_train(model, train_dataloader, val_dataloader, CFG['LEARNING_RATE'], CFG['EPOCHS'], 'kclue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_predictions(model, loader):\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    type_probs, polarity_probs, tense_probs, clarity_probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_input, _, _, _, _ in tqdm(loader):\n",
    "            attention_mask = data_input['attention_mask'].to(device)\n",
    "            input_ids = data_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "\n",
    "            type_output, polarity_output, tense_output, clarity_output = model(input_ids, attention_mask)\n",
    "            type_probs.append(type_output)\n",
    "            polarity_probs.append(polarity_output)\n",
    "            tense_probs.append(tense_output)\n",
    "            clarity_probs.append(clarity_output)\n",
    "    \n",
    "    return torch.cat(type_probs).cpu().detach().numpy(), \\\n",
    "            torch.cat(polarity_probs).cpu().detach().numpy(), \\\n",
    "            torch.cat(tense_probs).cpu().detach().numpy(), \\\n",
    "            torch.cat(clarity_probs).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model/kclue.pt\")\n",
    "test_dataloader = DataLoader(SentenceTypeDataset(test, tokenizer), batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355/355 [00:04<00:00, 78.11it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred_type, test_pred_polarity, test_pred_tense, test_pred_certainty = get_type_predictions(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_pred_tense[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_type = ['대화형' if i==0 else '사실형' if i==1 else '예측형' if i==2 else '추론형' for i in [np.argmax(p) for p in test_pred_type]]\n",
    "test_polarity = ['긍정' if i==0 else '미정' if i==1 else '부정' for i in [np.argmax(p) for p in test_pred_polarity]]\n",
    "test_tense = ['과거' if i==0 else '미래' if i==1 else '현재' for i in [np.argmax(p) for p in test_pred_tense]]\n",
    "test_certainty = ['불확실' if i==0 else '확실' for i in [np.argmax(p) for p in test_pred_certainty]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sum = []\n",
    "for i in range(len(test_type)):\n",
    "    label_sum.append(f'{test_type[i]}-{test_polarity[i]}-{test_tense[i]}-{test_certainty[i]}')\n",
    "\n",
    "submission['label'] = label_sum\n",
    "submission.to_csv('submission_weight4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7085</th>\n",
       "      <td>TEST_7085</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7086</th>\n",
       "      <td>TEST_7086</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7087</th>\n",
       "      <td>TEST_7087</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>TEST_7088</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>TEST_7089</td>\n",
       "      <td>사실형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7090 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID         label\n",
       "0     TEST_0000  사실형-부정-현재-확실\n",
       "1     TEST_0001  사실형-부정-현재-확실\n",
       "2     TEST_0002  사실형-부정-현재-확실\n",
       "3     TEST_0003  사실형-부정-현재-확실\n",
       "4     TEST_0004  사실형-부정-현재-확실\n",
       "...         ...           ...\n",
       "7085  TEST_7085  사실형-부정-현재-확실\n",
       "7086  TEST_7086  사실형-부정-현재-확실\n",
       "7087  TEST_7087  사실형-부정-현재-확실\n",
       "7088  TEST_7088  사실형-부정-현재-확실\n",
       "7089  TEST_7089  사실형-부정-현재-확실\n",
       "\n",
       "[7090 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stargan-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56ac9e226391ff4b9bb97a7266f7de8aac6a0147f12072446547a44f5facdf49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
